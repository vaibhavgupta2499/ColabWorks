{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow_DevIncept.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO0yMJHmiKeU7u8dYIbWZYh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaibhavgupta2499/ColabWorks/blob/main/Tensorflow_DevIncept.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW_c-kPpdtIk"
      },
      "source": [
        "![DevIncept](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse4.mm.bing.net%2Fth%3Fid%3DOIP.Aa1Ojw7Azjcpr51sEtQSJgAAAA%26pid%3DApi&f=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7J7jOIhXhRu"
      },
      "source": [
        "1. **Tensorflow - *Yugal Agarwal***\n",
        "   * About Tensorflow\n",
        "   * Installation (Import)\n",
        "   * Introduction to Tensors\n",
        "2. **Machine Learning - *Vaibhav Gupta***\n",
        "   * Linear Regression\n",
        "   * Classification\n",
        "   * Hidden Markov Models\n",
        "3. **Deep Learning - *Sai Dileep Kumar Mukkamala***\n",
        "   * What is Keras\n",
        "   * Neural Networks\n",
        "   * Convolutional neural network (CNN)\n",
        "   * CNN Code Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hhIdE6ia7nb"
      },
      "source": [
        "# Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CQ6K2enFoUG"
      },
      "source": [
        "## **AboutTenserFlow**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbpNrDthHOhE"
      },
      "source": [
        "TensorFlow is a free and open-source software library for machine learning. It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks. Tensorflow is a symbolic math library based on dataflow and differentiable programming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XksKwA2tI4d4"
      },
      "source": [
        "[![Teserflow](https://i.ytimg.com/vi/yjprpOoH5c8/maxresdefault.jpg)](https://www.tensorflow.org)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFu43KyJMve-"
      },
      "source": [
        "### **Why Tenserflow**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xEdXorFM4iZ"
      },
      "source": [
        "TensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y533TfcrNIuB"
      },
      "source": [
        "## **Installation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jODZ0vX6OWxg"
      },
      "source": [
        "In this section we will understand how to install Tenserflow in your system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaK0JcyhQlv3"
      },
      "source": [
        "#### **System Requirements**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro2SQerAQzwV"
      },
      "source": [
        "* Python 3.6 - 3.9\n",
        "* pip 19.0 or later\n",
        "* Ubuntu 16.04 or later (64-bit)\n",
        "* macOS 10.12.6 (Sierra) or later (64-bit)\n",
        "* Windows 7 or later (64-bit) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcyI4R7UTjhN"
      },
      "source": [
        "### **1. Install Python environment on your system**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd1KKRO-kVP2"
      },
      "source": [
        "Requires Python 3.6 - 3.9, pip and venv >= 19.0\n",
        "\n",
        "If these are already installed, skip to the next step.  \n",
        "Otherwise, install [Python](https://www.python.org/), the [pip package manager](https://pip.pypa.io/en/stable/installation/) and [venv](https://docs.python.org/3/library/venv.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQeZqgtBqvbj"
      },
      "source": [
        "### **2. Create a virtual environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm6MQQZEq_kf"
      },
      "source": [
        "Virtual Environment are used to isolate package installation from the system.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td5wm7tysuqG"
      },
      "source": [
        "#### **Windows**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSDIjgtks3Hl"
      },
      "source": [
        "Create a new virtual environment by chossing a python interpretor and making a **.\\venv** directory to hold it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVy0GxhHthzN"
      },
      "source": [
        "python -m venv --system-site-packages .\\venv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CaiPsDGv0ZG"
      },
      "source": [
        "Activate the virtual environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKun-Z4QwHxR"
      },
      "source": [
        ".\\venv\\Scripts\\activate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX6w1JM4wK0y"
      },
      "source": [
        "And exit the virtual environment later "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s653VCYawXG8"
      },
      "source": [
        "deactivate  # don't exit until you're done using TensorFlow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1H45U08wggt"
      },
      "source": [
        "### **3. Install the TenserFlow pip package**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bQQlXuswsgj"
      },
      "source": [
        "Choose one of the following TensorFlow packages to install from [PyPI](https://pypi.org/project/tensorflow/) :  \n",
        "* **tenserflow** - Latest stable release with CPU and GPU support (Ubuntu and Windows) .  \n",
        "* **tf - nighlty** - Preview build (unstable) . Ubuntu and Windows include GPU support .  \n",
        "* **tenserflow==1.5** - The final version of TenserFlow 1.x."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2fKFUnS3l7H"
      },
      "source": [
        "##  **Introduction To Tensors**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3tj4DYw3t6_"
      },
      "source": [
        "By programming perspective **Tensors** are multi-dimensional arrays with a uniform type (called a **dtype**).  \n",
        "![](https://media.geeksforgeeks.org/wp-content/uploads/two-d.png)  \n",
        "The above image is a simple 2-dimensional array.\n",
        "When we go in more fundamental mathematics perspective **tensor** is a generalization of scalar vector and matrix. \n",
        "![](https://miro.medium.com/max/891/0*jGB1CGQ9HdeUwlgB)  \n",
        "For eg. a vector is a one-dimensional tensor, and matrix is a two-dimensional matrix.\n",
        "\n",
        "If you're familiar with **NumPy**, tensors are (kind of) like **np.arrays**.\n",
        "\n",
        "All tensors are immutable like Python numbers and strings: you can never update the contents of a tensor, only create a new one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS0Sfzib4nnG"
      },
      "source": [
        "### Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auSw-mha42Xl"
      },
      "source": [
        "Lets create some basic tensors.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DcKDq8t-Sn6"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5t06VfTBDln"
      },
      "source": [
        "### **Initialization of Tensor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDDDtbvxBIpe"
      },
      "source": [
        "# Tensor Initialization\n",
        "x = tf.constant(5)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxtfVW9VBo-u"
      },
      "source": [
        "Above tensor is of scalar value which has no shape and is of data type **integer**.  \n",
        "We can also mention data type and shape. Lets see below example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQr6ahkxCCG1"
      },
      "source": [
        "y = tf.constant(7, shape=(1,1), dtype=tf.float32)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ybiwTb_CMyH"
      },
      "source": [
        "Now you can see the above tensor is also of scalar value with [1,1] shape and is of data type **float**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWshsmdBCZpt"
      },
      "source": [
        "y = tf.constant([[2,4,6],[4,8,9]])\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tcm-Zwi5EGUK"
      },
      "source": [
        "Now the above is a matrix of order [2 x 3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSqfhDtOE8BF"
      },
      "source": [
        "### **Intialization Methods**  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQq5kvBBFtVK"
      },
      "source": [
        "x = tf.ones((3,3))\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNhR3eQVGF5g"
      },
      "source": [
        "This will create a 3x3 matrix of 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aydfIbImGKH3"
      },
      "source": [
        "x = tf.zeros([2,3])\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lBu9lHJGUPw"
      },
      "source": [
        "This will create a 2x3 matrix containing all zeroes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2pESivCGZRy"
      },
      "source": [
        "x = tf.eye(3)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SU4wKL2Gx6O"
      },
      "source": [
        "This will print identity matrix of the mentioned order"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIZREQCoG10L"
      },
      "source": [
        "x = tf.range(start=1, limit=10, delta=2)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgq_qjiOHt8d"
      },
      "source": [
        "This will print range similar to python. In the above case it has jump of 2 after every number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-aEjQd8H4Sf"
      },
      "source": [
        "x = tf.cast(x, dtype=tf.float64)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kurjoRMNIVhA"
      },
      "source": [
        "This way we can convert data type into a specific data type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulo3lvt_InY0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKGXMqQgbuQo"
      },
      "source": [
        "# Machine Leanring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbdnKlcuMM4u"
      },
      "source": [
        "##Linear Regression\n",
        "Linear regression is one of the most basic forms of machine learning and is used to predict numeric values. In this tutorial we will use a linear model to predict the survival rate of passangers from the titanic dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsJ-Iov1P4RK"
      },
      "source": [
        "###How it Works\n",
        "Linear regression follows a very simple concept. If data points are related linearly, we can generate a line of best fit for these points and use it to predict future values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4Hp-dYmR2jf"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "x = [1, 2, 2.5, 3, 4]\n",
        "y = [1, 4, 7, 9, 15]\n",
        "plt.plot(x, y, 'ro')\n",
        "plt.axis([0, 6, 0, 20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m1DTp22SLo3"
      },
      "source": [
        "We can see that this data has a linear corresponding property. When the x value increases, y also increases. Because of this relation we can create a line of best fit for this dataset. In this example our line will only use one input variable, as we are working with two dimensions. In larger datasets with more features our line will have more features and inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv5eKLP_UYZi"
      },
      "source": [
        "plt.plot(x, y, 'ro')\n",
        "plt.axis([0, 6, 0, 20])\n",
        "plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd3T50PLB4lV"
      },
      "source": [
        "Once we've generated this line for our dataset, we can use its equation to predict future values. We just pass the features of the data point we would like to predict into the equation of the line and use the output as our prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y02FbC56Nbx0"
      },
      "source": [
        "### Setup and Imports\n",
        "Before we get started we must install *sklearn* and import the following modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4xpuxwKHOGL"
      },
      "source": [
        "!pip install -q sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI3zi2ZhQ3WB"
      },
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcII_xj9Ntyo"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "\n",
        "import tensorflow.compat.v2.feature_column as fc\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GltdTjiERfWi"
      },
      "source": [
        "### Data\n",
        "The dataset we will be focusing on here is the titanic dataset. It has tons of information about each passanger on the ship. Our first step is always to understand the data and explore it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpllWsKIOGOy"
      },
      "source": [
        "# Load dataset.\n",
        "dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') # training data\n",
        "dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data\n",
        "y_train = dftrain.pop('survived')\n",
        "y_eval = dfeval.pop('survived')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PJr5GosQBeY"
      },
      "source": [
        "The ```pd.read_csv()``` method will return to us a new pandas *dataframe*. You can think of a dataframe like a table. In fact, we can actually have a look at the table representation.\n",
        "\n",
        "We've decided to pop the \"survived\" column from our dataset and store it in a new variable. This column simply tells us if the person survived our not.\n",
        "\n",
        "To look at the data we'll use the ```.head()``` method from pandas. This will show us the first 5 items in our dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKsXeWinQiVR"
      },
      "source": [
        "dftrain.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeVQGdpCRC1P"
      },
      "source": [
        "And if we want a more statistical analysis of our data we can use the ```.describe()``` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkL2G42GRMf8"
      },
      "source": [
        "dftrain.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX7FSzrQRXeC"
      },
      "source": [
        "And since we talked so much about shapes in the previous tutorial let's have a look at that too!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR1Oy1dISdjn"
      },
      "source": [
        "dftrain.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIfLiSRMTeW4"
      },
      "source": [
        "So have have 627 entries and 9 features, nice!\n",
        "\n",
        "Now let's have a look at our survival information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX1lKW7TTh-E"
      },
      "source": [
        "y_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Edndbw4sU5Wd"
      },
      "source": [
        "dftrain.age.hist(bins=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SM_tYvyUtsw"
      },
      "source": [
        "dftrain.sex.value_counts().plot(kind='barh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCv3ek2LU1Lw"
      },
      "source": [
        "dftrain['class'].value_counts().plot(kind='barh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4kPWqBYVDlj"
      },
      "source": [
        "pd.concat([dftrain, y_train], axis=1).groupby('sex').survived.mean().plot(kind='barh').set_xlabel('% survive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l42qmk_bVHvD"
      },
      "source": [
        "After analyzing this information, we should notice the following:\n",
        "- Most passengers are in their 20's or 30's \n",
        "- Most passengers are male\n",
        "- Most passengers are in \"Third\" class\n",
        "- Females have a much higher chance of survival\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIBZww6kOIAp"
      },
      "source": [
        "### Training vs Testing Data\n",
        "You may have noticed that we loaded **two different datasets** above. This is because when we train models, we need two sets of data: **training and testing**. \n",
        "\n",
        "The **training** data is what we feed to the model so that it can develop and learn. It is usually a much larger size than the testing data.\n",
        "\n",
        "The **testing** data is what we use to evaulate the model and see how well it is performing. We must use a seperate set of data that the model has not been trained on to evaluate it. Can you think of why this is?\n",
        "\n",
        "Well, the point of our model is to be able to make predictions on NEW data, data that we have never seen before. If we simply test the model on the data that it has already seen we cannot measure its accuracy accuratly. We can't be sure that the model hasn't simply memorized our training data. This is why we need our testing and training data to be seperate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar_cXv2jV8A3"
      },
      "source": [
        "###Feature Columns\n",
        "In our dataset we have two different kinds of information: **Categorical and Numeric**\n",
        "\n",
        "Our **categorical data** is anything that is not numeric! For example, the sex column does not use numbers, it uses the words \"male\" and \"female\".\n",
        "\n",
        "Before we continue and create/train a model we must convet our categorical data into numeric data. We can do this by encoding each category with an integer (ex. male = 1, female = 2). \n",
        "\n",
        "Fortunately for us TensorFlow has some tools to help!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lcnwG0VXF5h"
      },
      "source": [
        "CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',\n",
        "                       'embark_town', 'alone']\n",
        "NUMERIC_COLUMNS = ['age', 'fare']\n",
        "\n",
        "feature_columns = []\n",
        "for feature_name in CATEGORICAL_COLUMNS:\n",
        "  vocabulary = dftrain[feature_name].unique()  # gets a list of all unique values from given feature column\n",
        "  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\n",
        "\n",
        "for feature_name in NUMERIC_COLUMNS:\n",
        "  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n",
        "\n",
        "print(feature_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-nazdZpXgAr"
      },
      "source": [
        "Essentially what we are doing here is creating a list of features that are used in our dataset. \n",
        "\n",
        "The cryptic lines of code inside the ```append()``` create an object that our model can use to map string values like \"male\" and \"female\" to integers. This allows us to avoid manually having to encode our dataframes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQlXWErlbhsG"
      },
      "source": [
        "###The Training Process\n",
        "So, we are almost done preparing our dataset and I feel as though it's a good time to explain how our model is trained. Specifically, how input data is fed to our model. \n",
        "\n",
        "For this specific model data is going to be streamed into it in small batches of 32. This means we will not feed the entire dataset to our model at once, but simply small batches of entries. We will feed these batches to our model multiple times according to the number of **epochs**. \n",
        "\n",
        "An **epoch** is simply one stream of our entire dataset. The number of epochs we define is the amount of times our model will see the entire dataset. We use multiple epochs in hope that after seeing the same data multiple times the model will better determine how to estimate it.\n",
        "\n",
        "Ex. if we have 10 ephocs, our model will see the same dataset 10 times. \n",
        "\n",
        "Since we need to feed our data in batches and multiple times, we need to create something called an **input function**. The input function simply defines how our dataset will be converted into batches at each epoch.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO0mBu_WaVXp"
      },
      "source": [
        "###Input Function\n",
        "The TensorFlow model we are going to use requires that the data we pass it comes in as a ```tf.data.Dataset``` object. This means we must create a *input function* that can convert our current pandas dataframe into that object. \n",
        "\n",
        "Relevant TensorFlow documentation - https://www.tensorflow.org/tutorials/estimator/linear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3qcbvOYbIwa"
      },
      "source": [
        "def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n",
        "  def input_function():  # inner function, this will be returned\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))  # create tf.data.Dataset object with data and its label\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(1000)  # randomize order of data\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)  # split dataset into batches of 32 and repeat process for number of epochs\n",
        "    return ds  # return a batch of the dataset\n",
        "  return input_function  # return a function object for use\n",
        "\n",
        "train_input_fn = make_input_fn(dftrain, y_train)  # here we will call the input_function that was returned to us to get a dataset object we can feed to the model\n",
        "eval_input_fn = make_input_fn(dfeval, y_eval, num_epochs=1, shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXqlPst9fpx4"
      },
      "source": [
        "###Creating the Model\n",
        "In this tutorial we are going to use a linear estimator to utilize the linear regression algorithm. \n",
        "\n",
        "Creating one is pretty easy! Have a look below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1Wo8brFf663"
      },
      "source": [
        "linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)\n",
        "# We create a linear estimtor by passing the feature columns we created earlier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5GPkW_CgDFy"
      },
      "source": [
        "###Training the Model\n",
        "Training the model is as easy as passing the input functions that we created earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J11OJrlZgPhb"
      },
      "source": [
        "linear_est.train(train_input_fn)  # train\n",
        "result = linear_est.evaluate(eval_input_fn)  # get model metrics/stats by testing on tetsing data\n",
        "\n",
        "clear_output()  # clears consoke output\n",
        "print(result['accuracy'])  # the result variable is simply a dict of stats about our model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LisxO81tgi1n"
      },
      "source": [
        "And we now we have a model with a 74% accuracy (this will change each time)! Not crazy impressive but decent for our first try.\n",
        "\n",
        "Now let's see how we can actually use this model to make predicitons.\n",
        "\n",
        "We can use the ```.predict()``` method to get survival probabilities from the model. This method will return a list of dicts that store a predicition for each of the entries in our testing data set. Below we've used some pandas magic to plot a nice graph of the predictions.\n",
        "\n",
        "As you can see the survival rate is not very high :/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQz0Lj60hjLI"
      },
      "source": [
        "pred_dicts = list(linear_est.predict(eval_input_fn))\n",
        "probs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\n",
        "\n",
        "probs.plot(kind='hist', bins=20, title='predicted probabilities')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN_r0Vn8VOf5"
      },
      "source": [
        "That's it for linear regression! Now onto classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG9gxhAqVTBT"
      },
      "source": [
        "##Classification\n",
        "Now that we've covered linear regression it is time to talk about classification. Where regression was used to predict a numeric value, classification is used to seperate data points into classes of different labels. In this example we will use a TensorFlow estimator to classify flowers.\n",
        "\n",
        "Since we've touched on how estimators work earlier, I'll go a bit quicker through this example. \n",
        "\n",
        "Documntation - https://www.tensorflow.org/tutorials/estimator/premade\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWk2Kb7Sdk-T"
      },
      "source": [
        "###Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH4_xJaD605_"
      },
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMiLc6LPdoPm"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zLNzmkGds1U"
      },
      "source": [
        "###Dataset\n",
        "This specific dataset seperates flowers into 3 different classes of species.\n",
        "- Setosa\n",
        "- Versicolor\n",
        "- Virginica\n",
        "\n",
        "The information about each flower is the following.\n",
        "- sepal length\n",
        "- sepal width\n",
        "- petal length\n",
        "- petal width"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puOQDTNKeCRC"
      },
      "source": [
        "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
        "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
        "# Lets define some constants to help us later on"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMW41Wd9eLIo"
      },
      "source": [
        "train_path = tf.keras.utils.get_file(\n",
        "    \"iris_training.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\")\n",
        "test_path = tf.keras.utils.get_file(\n",
        "    \"iris_test.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\")\n",
        "\n",
        "train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
        "test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n",
        "# Here we use keras (a module inside of TensorFlow) to grab our datasets and read them into a pandas dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHRWY47ecdr"
      },
      "source": [
        "Let's have a look at our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ9uo6KkegBH"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PzWyoE9eu8H"
      },
      "source": [
        "Now we can pop the species column off and use that as our label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP_nlslke4U8"
      },
      "source": [
        "train_y = train.pop('Species')\n",
        "test_y = test.pop('Species')\n",
        "train.head() # the species column is now gone"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oVw2zRkfTXq"
      },
      "source": [
        "train.shape  # we have 120 entires with 4 features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6ZWVZs8fg-H"
      },
      "source": [
        "###Input Function\n",
        "Remember that nasty input function we created earlier. Well we need to make another one here! Fortunatly for us this one is a little easier to digest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-NQRLv_fyhg"
      },
      "source": [
        "def input_fn(features, labels, training=True, batch_size=256):\n",
        "    # Convert the inputs to a Dataset.\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
        "\n",
        "    # Shuffle and repeat if you are in training mode.\n",
        "    if training:\n",
        "        dataset = dataset.shuffle(1000).repeat()\n",
        "    \n",
        "    return dataset.batch(batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL--3OAnf4mO"
      },
      "source": [
        "###Feature Columns\n",
        "And you didn't think we forgot about the feature columns, did you?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nErIJJbggQ5w"
      },
      "source": [
        "# Feature columns describe how to use the input.\n",
        "my_feature_columns = []\n",
        "for key in train.keys():\n",
        "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
        "print(my_feature_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kl1Wr_Xgpmv"
      },
      "source": [
        "###Building the Model\n",
        "And now we are ready to choose a model. For classification tasks there are variety of different estimators/models that we can pick from. Some options are listed below.\n",
        "- ```DNNClassifier``` (Deep Neural Network)\n",
        "- ```LinearClassifier```\n",
        "\n",
        "We can choose either model but the DNN seems to be the best choice. This is because we may not be able to find a linear coorespondence in our data. \n",
        "\n",
        "So let's build a model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7YVQowgiDak"
      },
      "source": [
        "# Build a DNN with 2 hidden layers with 30 and 10 hidden nodes each.\n",
        "classifier = tf.estimator.DNNClassifier(\n",
        "    feature_columns=my_feature_columns,\n",
        "    # Two hidden layers of 30 and 10 nodes respectively.\n",
        "    hidden_units=[30, 10],\n",
        "    # The model must choose between 3 classes.\n",
        "    n_classes=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ_SJAMuiF6p"
      },
      "source": [
        "What we've just done is created a deep neural network that has two hidden layers. These layers have 30 and 10 neurons respectively. This is the number of neurons the TensorFlow official tutorial uses so we'll stick with it. However, it is worth mentioning that the number of hidden neurons is an arbitrary number and many experiments and tests are usually done to determine the best choice for these values. Try playing around with the number of hidden neurons and see if your results change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBHnWYKTjV5D"
      },
      "source": [
        "###Training\n",
        "Now it's time to train the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INug63pCjaOw"
      },
      "source": [
        "classifier.train(\n",
        "    input_fn=lambda: input_fn(train, train_y, training=True),\n",
        "    steps=5000)\n",
        "# We include a lambda to avoid creating an inner function previously"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57oNBLV1j0wc"
      },
      "source": [
        "The only thing to explain here is the **steps** argument. This simply tells the classifier to run for 5000 steps. Try modifiying this and seeing if your results change. Keep in mind that more is not always better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5suI1lmskE7p"
      },
      "source": [
        "###Evaluation\n",
        "Now let's see how this trained model does!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23rIrgbxkJUO"
      },
      "source": [
        "eval_result = classifier.evaluate(\n",
        "    input_fn=lambda: input_fn(test, test_y, training=False))\n",
        "\n",
        "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v1ZMe7jkXdp"
      },
      "source": [
        "Notice this time we didn't specify the number of steps. This is because during evaluation the model will only look at the testing data one time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "464HkZ6lknua"
      },
      "source": [
        "### Predictions\n",
        "Now that we have a trained model it's time to use it to make predictions. I've written a little script below that allows you to type the features of a flower and see a prediction for its class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQRLq4M1k1jm"
      },
      "source": [
        "def input_fn(features, batch_size=256):\n",
        "    # Convert the inputs to a Dataset without labels.\n",
        "    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n",
        "\n",
        "features = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\n",
        "predict = {}\n",
        "\n",
        "print(\"Please type numeric values as prompted.\")\n",
        "for feature in features:\n",
        "  valid = True\n",
        "  while valid: \n",
        "    val = input(feature + \": \")\n",
        "    if not val.isdigit(): valid = False\n",
        "\n",
        "  predict[feature] = [float(val)]\n",
        "\n",
        "predictions = classifier.predict(input_fn=lambda: input_fn(predict))\n",
        "for pred_dict in predictions:\n",
        "    class_id = pred_dict['class_ids'][0]\n",
        "    probability = pred_dict['probabilities'][class_id]\n",
        "\n",
        "    print('Prediction is \"{}\" ({:.1f}%)'.format(\n",
        "        SPECIES[class_id], 100 * probability))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tRxhpmSr1FH"
      },
      "source": [
        "# Here is some example input and expected classes you can try above\n",
        "expected = ['Setosa', 'Versicolor', 'Virginica']\n",
        "predict_x = {\n",
        "    'SepalLength': [5.1, 5.9, 6.9],\n",
        "    'SepalWidth': [3.3, 3.0, 3.1],\n",
        "    'PetalLength': [1.7, 4.2, 5.4],\n",
        "    'PetalWidth': [0.5, 1.5, 2.1],\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ9iJrSbBTZB"
      },
      "source": [
        "##Hidden Markov Models\n",
        "\n",
        "\"The Hidden Markov Model is a finite set of states, each of which is associated with a (generally multidimensional) probability distribution. Transitions among the states are governed by a set of probabilities called transition probabilities.\" (http://jedlik.phy.bme.hu/~gerjanos/HMM/node4.html)\n",
        "\n",
        "A hidden markov model works with probabilities to predict future events or states. In this section we will learn how to create a hidden markov model that can predict the weather.\n",
        "\n",
        "Documentation -  https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/HiddenMarkovModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKJSFk4NP0eq"
      },
      "source": [
        "###Data\n",
        "Let's start by discussing the type of data we use when we work with a hidden markov model. \n",
        "\n",
        "In the previous sections we worked with large datasets of 100's of different entries. For a markov model we are only interested in probability distributions that have to do with states. \n",
        "\n",
        "We can find these probabilities from large datasets or may already have these values. We'll run through an example in a second that should clear some things up, but let's discuss the components of a markov model.\n",
        "\n",
        "**States:** In each markov model we have a finite set of states. These states could be something like \"warm\" and \"cold\" or \"high\" and \"low\" or even \"red\", \"green\" and \"blue\". These states are \"hidden\" within the model, which means we do not direcly observe them.\n",
        "\n",
        "**Observations:** Each state has a particular outcome or observation associated with it based on a probability distribution. An example of this is the following: *On a hot day Tim has a 80% chance of being happy and a 20% chance of being sad.*\n",
        "\n",
        "**Transitions:** Each state will have a probability defining the likelyhood of transitioning to a different state. An example is the following: *a cold day has a 30% chance of being followed by a hot day and a 70% chance of being follwed by another cold day.*\n",
        "\n",
        "To create a hidden markov model we need.\n",
        "- States\n",
        "- Observation Distribution\n",
        "- Transition Distribution\n",
        "\n",
        "For our purpose we will assume we already have this information available as we attempt to predict the weather on a given day."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK2QbOzr6jNJ"
      },
      "source": [
        "###Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Suf1v8kJ6niA"
      },
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN_Fkrx30xbb"
      },
      "source": [
        "Due to a version mismatch with tensorflow v2 and tensorflow_probability we need to install the most recent version of tensorflow_probability (see below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kawrMHKGBWyS"
      },
      "source": [
        "!pip install tensorflow_probability==0.8.0rc0 --user --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEIk7FYD6lcF"
      },
      "source": [
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssOcn-nIOCcV"
      },
      "source": [
        "###Weather Model\n",
        "We will model a simple weather system and try to predict the temperature on each day given the following information.\n",
        "1. Cold days are encoded by a 0 and hot days are encoded by a 1.\n",
        "2. The first day in our sequence has an 80% chance of being cold.\n",
        "3. A cold day has a 30% chance of being followed by a hot day.\n",
        "4. A hot day has a 20% chance of being followed by a cold day.\n",
        "5. On each day the temperature is\n",
        " normally distributed with mean and standard deviation 0 and 5 on\n",
        " a cold day and mean and standard deviation 15 and 10 on a hot day.\n",
        "\n",
        "If you're unfamiliar with **standard deviation** it can be put simply as the range of expected values. \n",
        "\n",
        "In this example, on a hot day the average temperature is 15 and ranges from 5 to 25.\n",
        "\n",
        "To model this in TensorFlow we will do the following.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LBLEJp4YlIf"
      },
      "source": [
        "tfd = tfp.distributions  # making a shortcut for later on\n",
        "initial_distribution = tfd.Categorical(probs=[0.2, 0.8])  # Refer to point 2 above\n",
        "transition_distribution = tfd.Categorical(probs=[[0.5, 0.5],\n",
        "                                                 [0.2, 0.8]])  # refer to points 3 and 4 above\n",
        "observation_distribution = tfd.Normal(loc=[0., 15.], scale=[5., 10.])  # refer to point 5 above\n",
        "\n",
        "# the loc argument represents the mean and the scale is the standard devitation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XtTg0l04mqc"
      },
      "source": [
        "We've now created distribution variables to model our system and it's time to create the hidden markov model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4M6cZww4mZk"
      },
      "source": [
        "model = tfd.HiddenMarkovModel(\n",
        "    initial_distribution=initial_distribution,\n",
        "    transition_distribution=transition_distribution,\n",
        "    observation_distribution=observation_distribution,\n",
        "    num_steps=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ0XIA2M5gqD"
      },
      "source": [
        "The number of steps represents the number of days that we would like to predict information for. In this case we've chosen 7, an entire week.\n",
        "\n",
        "To get the **expected temperatures** on each day we can do the following."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plVVG4fi55Jv"
      },
      "source": [
        "mean = model.mean()\n",
        "\n",
        "# due to the way TensorFlow works on a lower level we need to evaluate part of the graph\n",
        "# from within a session to see the value of this tensor\n",
        "\n",
        "# in the new version of tensorflow we need to use tf.compat.v1.Session() rather than just tf.Session()\n",
        "with tf.compat.v1.Session() as sess:  \n",
        "  print(mean.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-byadvwb0jb"
      },
      "source": [
        "# Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v9ZQYEUa7Vp"
      },
      "source": [
        "## What is Keras?\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W6-MwkEpmEp"
      },
      "source": [
        ">* To Understand about that , First we need to know about Deep Learning.\n",
        ">\n",
        ">* Deep learning is a machine learning technique that teaches computers to do what comes naturally to humans: learn by example. Deep learning is a key technology behind driverless cars, enabling them to recognize a stop sign, or to distinguish a pedestrian from a lamppost. It is the key to voice control in consumer devices like phones, tablets, TVs, and hands-free speakers. Deep learning is getting lots of attention lately and for good reason. Its achieving results that were not possible before.\n",
        ">\n",
        ">* In deep learning, a computer model learns to perform classification tasks directly from images, text, or sound. Deep learning models can achieve state-of-the-art accuracy, sometimes exceeding human-level performance. Models are trained by using a large set of labeled data and neural network architectures that contain many layers.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhUwQwrAa7Vr"
      },
      "source": [
        "![Alt text](https://databricks.com/wp-content/uploads/2019/04/logo-keras.png)\n",
        "### Keras Deep Learning library with TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh9wFghBa7Vr"
      },
      "source": [
        ">Keras is a minimalist, highly modular neural networks library, written in Python and capable of running on top of either TensorFlow or Theano. \n",
        ">\n",
        ">It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
        "ref: https://keras.io/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHZoaq8ea7Vs"
      },
      "source": [
        "#### Why Keras?\n",
        "\n",
        ">Keras is an API designed for human beings, not machines. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear and actionable feedback upon user error.\n",
        "    >\n",
        ">This makes Keras easy to learn and easy to use. As a Keras user, you are more productive, allowing you to try more ideas than your competition, faster -- which in turn helps you win machine learning competitions.\n",
        "    >\n",
        ">This ease of use does not come at the cost of reduced flexibility: because Keras integrates with lower-level deep learning languages (in particular TensorFlow), it enables you to implement anything you could have built in the base language. In particular, as tf.keras, the Keras API integrates seamlessly with your TensorFlow workflows.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OqKHZi_a7Vt"
      },
      "source": [
        "#### Keras Working Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e09DMramgJUd"
      },
      "source": [
        "![Alt text](https://blog.keras.io/img/keras-tensorflow-logo.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmR1CNXUa7Vu"
      },
      "source": [
        "### MODEL Definition\n",
        "There are two types of models available in Keras: the Sequential model and the Model class used with functional API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5f7Gt30a7Vu"
      },
      "source": [
        "#### Sequential Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFqVNZ0Ba7Vu"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "The simplest model is defined in the Sequential class which is a linear stack of Layers. You can create a Sequential model and define all of the layers in the constructor, for example: \n",
        "       \n",
        "       from keras.models import Sequential\n",
        "       model = Sequential(...)\n",
        "\n",
        "A more useful idiom is to create a Sequential model and add your layers in the order of the computation you wish to perform, for example:       \n",
        "       \n",
        "       from keras.models import Sequential\n",
        "       model = Sequential()\n",
        "       model.add(...)\n",
        "       model.add(...)\n",
        "       model.add(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkjVjWkua7Vv"
      },
      "source": [
        "#### Functional API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mZ-i-HYa7Vv"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "The Keras functional API provides a more flexible way for defining models.\n",
        "\n",
        "It specifically allows you to define multiple input or output models as well as models that share layers. More than that, it allows you to define ad hoc acyclic network graphs.\n",
        "\n",
        "Models are defined by creating instances of layers and connecting them directly to each other in pairs, then defining a Model that specifies the layers to act as the input and output to the model,For Example:\n",
        "\n",
        "                         inputs = Input(shape=(3,))\n",
        "                         x = Dense(50, activation='relu')(inputs)\n",
        "                         output = Dense(1, activation = 'sigmoid')(x)\n",
        "                         n_net = Model(inputs, output)\n",
        "                         n_net.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "                         n_net.fit(x=dat_train, y=y_classifier_train, epochs=10,\n",
        "                         verbose=1, validation_data=(dat_test, y_classifier_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-I467a8a7Vv"
      },
      "source": [
        "### Model Compilation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHfyPrJ5a7Vw"
      },
      "source": [
        ">Before training a model, you need to configure the learning process, which is done via the compile method. It receives three arguments:\n",
        ">\n",
        ">* An optimizer. This could be the string identifier of an existing optimizer (such as rmsprop or adagrad), or an instance of the Optimizer class. See: optimizers.\n",
        ">* A loss function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as categorical_crossentropy or mse), or it can be an objective function. See: losses.\n",
        ">* A list of metrics. For any classification problem you will want to set this to metrics=['accuracy']. A metric could be the string identifier of an existing metric or a custom metric function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtE4MAVha7Vw"
      },
      "source": [
        "#### For a multi-class classification problem\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "### For a binary classification problem\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "### For a mean squared error regression problem\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='mse')\n",
        "\n",
        "### For custom metrics\n",
        "import keras.backend as K\n",
        "\n",
        "def mean_pred(y_true, y_pred):\n",
        "    return K.mean(y_pred)\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', mean_pred])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3y6tn8da7V2"
      },
      "source": [
        "### Applications\n",
        "Keras Applications are deep learning models that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning.\n",
        "\n",
        "Weights are downloaded automatically when instantiating a model. They are stored at ~/.keras/models/."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW__Rj-sg03d"
      },
      "source": [
        "## Neural Networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D2tTd7bg5ld"
      },
      "source": [
        "![NN](https://i2.wp.com/vinodsblog.com/wp-content/uploads/2019/01/recurrent-neural-networks-4259348843-1549021778666.png?resize=1300%2C650&ssl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Curz4PCzhBHM"
      },
      "source": [
        "Neural networks are artificial systems that were inspired by biological neural networks. These systems learn to perform tasks by being exposed to various datasets and examples without any task-specific rules. The idea is that the system generates identifying characteristics from the data they have been passed without being programmed with a pre-programmed understanding of these datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzHE8NyjiCOV"
      },
      "source": [
        "Neural networks are based on computational models for threshold logic. Threshold logic is a combination of algorithms and mathematics. Neural networks are based either on the study of the brain or on the application of neural networks to artificial intelligence. The work has led to improvements in finite automata theory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI1iaFKaiXcJ"
      },
      "source": [
        "Components of a typical neural network involve neurons, connections, weights, biases, propagation function, and a learning rule. Neurons will receive an input p_j(t) from predecessor neurons that have an activation a_j(t), threshold $\\theta$_j, an activation function f, and an output function f_{out}. Connections consist of connections, weights and biases which rules how neuron $i$ transfers output to neuron $j$. Propagation computes the input and outputs the output and sums the predecessor neurons function with the weight. The learning rule modifies the weights and thresholds of the variables in the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV2SOLsQiago"
      },
      "source": [
        "**Supervised vs Unsupervised Learning:**\n",
        "\n",
        "* Neural networks learn via supervised learning; \n",
        "\n",
        "* Supervised machine learning involves an input variable x and output variable y. The algorithm learns from a training dataset. With each correct answers, algorithms iteratively make predictions on the data. The learning stops when the algorithm reaches an acceptable level of performance.\n",
        "Unsupervised machine learning has input data X and no corresponding output variables. The goal is to model the underlying structure of the data for understanding more about the data. The keywords for supervised machine learning are classification and regression. For unsupervised machine learning, the keywords are clustering and association."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdmDr0Xdit9d"
      },
      "source": [
        "**Evolution of Neural Networks:**\n",
        "\n",
        "Hebbian learning deals with neural plasticity. Hebbian learning is unsupervised and deals with long term potentiation. Hebbian learning deals with pattern recognition and exclusive-or circuits; deals with if-then rules.\n",
        "\n",
        "Back propagation solved the exclusive-or issue that Hebbian learning could not handle. This also allowed for multi-layer networks to be feasible and efficient. If an error was found, the error was solved at each layer by modifying the weights at each node. This led to the development of support vector machines, linear classifiers, and max-pooling. The vanishing gradient problem affects feedforward networks that use back propagation and recurrent neural network. This is known as deep-learning.\n",
        "\n",
        "Hardware-based designs are used for biophysical simulation and neurotrophic computing. They have large scale component analysis and convolution creates new class of neural computing with analog. This also solved back-propagation for many-layered feedforward neural networks.\n",
        "\n",
        "Convolutional networks are used for alternating between convolutional layers and max-pooling layers with connected layers (fully or sparsely connected) with a final classification layer. The learning is done without unsupervised pre-training. Each filter is equivalent to a weights vector that has to be trained. The shift variance has to be guaranteed to dealing with small and large neural networks. This is being resolved in Development Networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9Yej_85jWg0"
      },
      "source": [
        "**Types of Neural Networks:**\n",
        "\n",
        "There are seven types of neural networks that can be used.\n",
        "\n",
        ">* The first is a multilayer perceptron which has three or more layers and uses a nonlinear activation function.\n",
        ">\n",
        ">* The second is the convolutional neural network that uses a variation of the multilayer perceptrons.\n",
        ">\n",
        ">* The third is the recursive neural network that uses weights to make structured predictions.\n",
        ">\n",
        ">* The fourth is a recurrent neural network that makes connections between the neurons in a directed cycle. The long short-term memory neural network uses the recurrent neural network architecture and does not use activation function.\n",
        ">\n",
        ">* The final two are sequence to sequence modules which uses two recurrent networks and shallow neural networks which produces a vector space from an amount of text. These neural networks are applications of the basic neural network demonstrated below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMTVC4jfk65V"
      },
      "source": [
        "## Convolutional neural network(CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir14WO2ZlBTD"
      },
      "source": [
        "* As we discussed about various types of Neural Networks.\n",
        "* CNN is one of those."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ3s5_AulPSV"
      },
      "source": [
        "**Convolutional neural network (ConvNets or CNNs) is one of the main categories to do images recognition, images classifications. Objects detections, recognition faces etc.,**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiT8ea_HleS6"
      },
      "source": [
        "![cnn](https://i1.wp.com/www.michaelchimenti.com/wp-content/uploads/2017/11/Deep-Neural-Network-What-is-Deep-Learning-Edureka.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z218TJ-UlvDp"
      },
      "source": [
        "***CNN image classifications takes an input image, process it and classify it under certain categories (Eg., Dog, Cat, Tiger, Lion). Computers sees an input image as array of pixels and it depends on the image resolution. Based on the image resolution, it will see h x w x d( h = Height, w = Width, d = Dimension ). Eg., An image of 6 x 6 x 3 array of matrix of RGB (3 refers to RGB values) and an image of 4 x 4 x 1 array of matrix of grayscale image.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD3FLUnCmDWT"
      },
      "source": [
        "**Technically, deep learning CNN models to train and test, each input image will pass it through a series of convolution layers with filters (Kernals), Pooling, fully connected layers (FC) and apply Softmax function to classify an object with probabilistic values between 0 and 1. The below figure is a complete flow of CNN to process an input image and classifies the objects based on values.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viL_nTeimJzj"
      },
      "source": [
        "![](https://miro.medium.com/max/1400/1*XbuW8WuRrAY5pC4t-9DZAQ.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqos-k5Jmd-M"
      },
      "source": [
        "**Convolution Layer**\n",
        "\n",
        "* Convolution is the first layer to extract features from an input image. Convolution preserves the relationship between pixels by learning image features using small squares of input data. It is a mathematical operation that takes two inputs such as image matrix and a filter or kernel.\n",
        "![](https://miro.medium.com/max/576/1*kYSsNpy0b3fIonQya66VSQ.png)\n",
        "* Consider a 5 x 5 whose image pixel values are 0, 1 and filter matrix 3 x 3 as shown in below\n",
        "![](https://miro.medium.com/max/516/1*4yv0yIH0nVhSOv3AkLUIiw.png)\n",
        "* Then the convolution of 5 x 5 image matrix multiplies with 3 x 3 filter matrix which is called Feature Map as output shown in below.\n",
        "![](https://miro.medium.com/max/335/1*MrGSULUtkXc0Ou07QouV8A.gif)\n",
        "* Convolution of an image with different filters can perform operations such as edge detection, blur and sharpen by applying filters. The below example shows various convolution image after applying different types of filters (Kernels).\n",
        "\n",
        "**Strides**\n",
        "* Stride is the number of pixels shifts over the input matrix. When the stride is 1 then we move the filters to 1 pixel at a time. When the stride is 2 then we move the filters to 2 pixels at a time and so on. The below figure shows convolution would work with a stride of 2.\n",
        "![](https://miro.medium.com/max/869/1*nGHLq1hx0gt02OK4l8WmRg.png)\n",
        "\n",
        "**Padding**\n",
        "\n",
        "Sometimes filter does not fit perfectly fit the input image. We have two options:\n",
        "* Pad the picture with zeros (zero-padding) so that it fits\n",
        "* Drop the part of the image where the filter did not fit. This is called valid padding which keeps only valid part of the image.\n",
        "\n",
        "**Non Linearity (ReLU)**\n",
        "\n",
        "* ReLU stands for Rectified Linear Unit for a non-linear operation. The output is **(x) = max(0,x)**.\n",
        "\n",
        "* Why ReLU is important : ReLUs purpose is to introduce non-linearity in our ConvNet. Since, the real world data would want our ConvNet to learn would be non-negative linear values.\n",
        "![](https://miro.medium.com/max/561/1*gcvuKm3nUePXwUOLXfLIMQ.png)\n",
        "* There are other non linear functions such as tanh or sigmoid that can also be used instead of ReLU. Most of the data scientists use ReLU since performance wise ReLU is better than the other two.\n",
        "\n",
        "**Pooling Layer**\n",
        "\n",
        "Pooling layers section would reduce the number of parameters when the images are too large. Spatial pooling also called subsampling or downsampling which reduces the dimensionality of each map but retains important information. Spatial pooling can be of different types:\n",
        "* Max Pooling\n",
        "* Average Pooling\n",
        "* Sum Pooling\n",
        "\n",
        "Max pooling takes the largest element from the rectified feature map. Taking the largest element could also take the average pooling. Sum of all elements in the feature map call as sum pooling.\n",
        "![](https://miro.medium.com/max/753/1*SmiydxM5lbTjoKWYPiuzWQ.png)\n",
        "\n",
        "**Fully Connected Layer**\n",
        "\n",
        "The layer we call as FC layer, we flattened our matrix into vector and feed it into a fully connected layer like a neural network.\n",
        "![](https://miro.medium.com/max/693/1*Mw6LKUG8AWQhG73H1caT8w.png)\n",
        "\n",
        "In the above diagram, the feature map matrix will be converted as vector (x1, x2, x3, ). With the fully connected layers, we combined these features together to create a model. Finally, we have an activation function such as softmax or sigmoid to classify the outputs as cat, dog, car, truck etc.,\n",
        "![](https://miro.medium.com/max/875/1*4GLv7_4BbKXnpc6BRb0Aew.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UFr3OtdpdFY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOV-VYXgqGRq"
      },
      "source": [
        "## CNN Code Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKPo1Zh1qJc3"
      },
      "source": [
        "**Problem Definition**\n",
        "\n",
        "**Recognize handwritten digits**\n",
        "\n",
        "![](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse3.mm.bing.net%2Fth%3Fid%3DOIP.nLmRwwdIph2KOrNlxF16vwAAAA%26pid%3DApi&f=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OKP7kjwqvMn"
      },
      "source": [
        "**Data**\n",
        "\n",
        "The MNIST database ([link](http://yann.lecun.com/exdb/mnist)) has a database of handwritten digits. \n",
        "\n",
        "The training set has $60,000$ samples. \n",
        "\n",
        "The test set has $10,000$ samples.\n",
        "\n",
        "The digits are size-normalized and centered in a fixed-size image. \n",
        "\n",
        "The data page has description on how the data was collected. It also has reports the benchmark of various algorithms on the test dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r26QHjf_q1S3"
      },
      "source": [
        "**Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYb653q7seYy"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "#download mnist data and split into train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z9nwlUHtiq-"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#plot the first image in the dataset\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AYUvo_PtpTU"
      },
      "source": [
        "#check image shape\n",
        "X_train[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrMgBACRtsQK"
      },
      "source": [
        "#reshape data to fit model\n",
        "X_train = X_train.reshape(60000,28,28,1)\n",
        "X_test = X_test.reshape(10000,28,28,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BThk08DXtvuY"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "#one-hot encode target column\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "y_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b06_8wEHuCTg"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten\n",
        "#create model\n",
        "model = Sequential()\n",
        "#add model layers\n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWlPLCxBuOWN"
      },
      "source": [
        "#compile model using accuracy to measure model performance\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4f4hlUVuRKf"
      },
      "source": [
        "#train the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L5CUT6ruVtg"
      },
      "source": [
        "#predict first 4 images in the test set\n",
        "model.predict(X_test[:4])\n",
        "#We can see that our model predicted 7, 2, 1 and 0 for the first four images."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "393pZD2MwYCC"
      },
      "source": [
        "#actual results for first 4 images in test set\n",
        "y_test[:4]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}